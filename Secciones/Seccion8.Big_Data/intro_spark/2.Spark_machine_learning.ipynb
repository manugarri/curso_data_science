{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-25T16:58:26+01:00\n",
      "\n",
      "CPython 3.6.8\n",
      "IPython 7.5.0\n",
      "\n",
      "compiler   : GCC 7.3.0\n",
      "system     : Linux\n",
      "release    : 4.15.0-50-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 8\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark tiene varios modulos, entre ellos un modulo de [Machine Learning](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html), vamos a ver como usarlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una sesion de spark, uso 6 cores por que si uso mas me quedo sin memoria en mi ordenador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.70:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[6]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Taxis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f35d8f32eb8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "     .master(\"local[6]\") \\\n",
    "     .appName(\"Taxis\") \\\n",
    "     .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a replicar el pipeline que usamos con dask\n",
    "\n",
    "```\n",
    "pipeline = make_pipeline(\n",
    "    make_union(\n",
    "        make_pipeline(\n",
    "            ColumnSelector(cols=[\"vendor_id\", \"store_and_fwd_flag\", \"payment_type\", \"rate_code\"]),\n",
    "            OneHotEncoder()\n",
    "        ),\n",
    "        make_pipeline(\n",
    "            ColumnSelector(cols=[\"pickup_longitude\", \"pickup_latitude\", \"passenger_count\"]),\n",
    "            StandardScaler()\n",
    "        )  \n",
    "    ),\n",
    "    SGDRegressor()\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_objetivo = \"tip_amount\"\n",
    "variables_independientes = [\"vendor_id\", \"store_and_fwd_flag\", \"payment_type\",\n",
    "               \"rate_code\", \"pickup_longitude\", \"pickup_latitude\", \n",
    "               \"passenger_count\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a leer el dataset de 2014 de trayectos de taxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = (\n",
    "    spark\n",
    "    .read\n",
    "    .parquet(\"../data/nyc_taxi_data_2014.parquet/\")\n",
    "    .select(variables_independientes + [variable_objetivo])\n",
    "    .withColumnRenamed(variable_objetivo, \"label\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(vendor_id='CMT', store_and_fwd_flag='N', payment_type='CRD', rate_code=1, pickup_longitude=-73.943106, pickup_latitude=40.706701, passenger_count=1, label=1.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark tiene la figura de [Pipelines](https://spark.apache.org/docs/latest/ml-pipeline.html) que es similar a la de scikit-learn, en particular define 2 tipos de objetos:\n",
    "\n",
    "- **Transformers**, que transforman el dataset de entrenamiento (encoders, standardizadores, etc)\n",
    "- **Estimators**, que realizan las prediciones (Regresion lineal, Random Forest, etc.\n",
    "\n",
    "Los transformadores de Spark funcionan con **1 sola columna como input** y **1 columna como output**.\n",
    "\n",
    "Los estimadores de spark funcionan con **1 columna como input** (llamada `features`) y **1 columna como output** (generalmente llamada `labels`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el pipeline vamos a usar los siguientes transformadores:\n",
    "    \n",
    "- `StringIndexer`, que convierte una columna de strings a numeros\n",
    "- `OneHotEncoder`, que realiza codificación one hot de enteros\n",
    "- `StandardScaler`, que estandariza un vector de numeros\n",
    "- `VectorAssembler`, que toma un conjunto de columnas y las agrupa en una sola columna de vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, StandardScaler, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_categoricas = [\"vendor_id\", \"store_and_fwd_flag\", \"payment_type\", \"rate_code\"]\n",
    "variables_numericas = [\"pickup_longitude\", \"pickup_latitude\", \"passenger_count\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "el StringIndexer y OneHotEncoder de spark trabajan con una sola columna, por lo tanto tenemos que crear  una lista de indexadores y encoders (uno de cada para cada columna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = []\n",
    "encoders = []\n",
    "for columna in variables_categoricas:\n",
    "    indexers.append(\n",
    "        StringIndexer(inputCol=columna, outputCol=f\"{columna}_idx)\", handleInvalid=\"keep\"))\n",
    "    encoders.append(\n",
    "        OneHotEncoder(inputCol=f\"{columna}_idx)\", outputCol=f\"{columna}_enc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_0355836cdcfd,\n",
       " StringIndexer_09e1ed4bde22,\n",
       " StringIndexer_b9584fc7d362,\n",
       " StringIndexer_884a41018097]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OneHotEncoder_70fb743ff741,\n",
       " OneHotEncoder_fb43e9837251,\n",
       " OneHotEncoder_c3962db20440,\n",
       " OneHotEncoder_f841bd12d78d]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vendor_id_idx)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoders[0].getInputCol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vendor_id_enc'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoders[0].getOutputCol()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos un [VectorAssembler](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler) que tomara el output de los encoders y los convertira en un vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensamblador_categorico = VectorAssembler(\n",
    "                            inputCols=[enc.getOutputCol() for enc in encoders], \n",
    "                            outputCol=\"vector_categorico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorAssembler_882866db619c"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensamblador_categorico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos un pipeline para datos categoricos, que va a ser una lista con los siguentes pasos (o `stages` como se llaman en spark):\n",
    "\n",
    "1. indexers, que convierte strings de variables categoricas a numeros\n",
    "\n",
    "2.encoders, que convierten strings a codificacion one hot\n",
    "\n",
    "3. ensamblador, que  cogen el output de distintos encoders y los unen en un vector unico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_categorico = Pipeline(stages=indexers +encoders + [ensamblador_categorico])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.pipeline.Pipeline"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pipeline_categorico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos evaluar el pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_cat = pipeline_categorico.fit(taxi).transform(taxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[vendor_id: string, store_and_fwd_flag: string, payment_type: string, rate_code: bigint, pickup_longitude: double, pickup_latitude: double, passenger_count: bigint, label: double, vendor_id_idx): double, store_and_fwd_flag_idx): double, payment_type_idx): double, rate_code_idx): double, vendor_id_enc: vector, store_and_fwd_flag_enc: vector, payment_type_enc: vector, rate_code_enc: vector, vector_categorico: vector]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(vendor_id='CMT', store_and_fwd_flag='N', payment_type='CRD', rate_code=1, pickup_longitude=-73.943106, pickup_latitude=40.706701, passenger_count=1, label=1.0, vendor_id_idx)=1.0, store_and_fwd_flag_idx)=0.0, payment_type_idx)=0.0, rate_code_idx)=0.0, vendor_id_enc=SparseVector(2, {1: 1.0}), store_and_fwd_flag_enc=SparseVector(2, {0: 1.0}), payment_type_enc=SparseVector(5, {0: 1.0}), rate_code_enc=SparseVector(12, {0: 1.0}), vector_categorico=SparseVector(21, {1: 1.0, 2: 1.0, 4: 1.0, 9: 1.0})),\n",
       " Row(vendor_id='CMT', store_and_fwd_flag='N', payment_type='CRD', rate_code=1, pickup_longitude=-74.006878, pickup_latitude=40.739494, passenger_count=1, label=2.4, vendor_id_idx)=1.0, store_and_fwd_flag_idx)=0.0, payment_type_idx)=0.0, rate_code_idx)=0.0, vendor_id_enc=SparseVector(2, {1: 1.0}), store_and_fwd_flag_enc=SparseVector(2, {0: 1.0}), payment_type_enc=SparseVector(5, {0: 1.0}), rate_code_enc=SparseVector(12, {0: 1.0}), vector_categorico=SparseVector(21, {1: 1.0, 2: 1.0, 4: 1.0, 9: 1.0}))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_cat.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables numéricas**\n",
    "\n",
    "Para las variables numericas simplemente creamos un standard scaler las variables numericas.\n",
    "\n",
    "El [StandardScaler](https://spark.apache.org/docs/latest/ml-features.html#standardscaler) de spark funciona con una lista de vectores, estandarizando cada variable. Por ello ponemos primero todas las variables numericas en un vector mediante el uso de un VectorAssembler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensamblador_numerico = VectorAssembler(inputCols=variables_numericas, \n",
    "                                       outputCol=\"vector_numerico\")    \n",
    "\n",
    "estandarizador = StandardScaler(inputCol=\"vector_numerico\", outputCol=\"vector_numerico_std\")    \n",
    "\n",
    "pipeline_numerico = Pipeline(stages=[ensamblador_numerico, estandarizador])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_num = pipeline_numerico.fit(taxi).transform(taxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(vendor_id='CMT', store_and_fwd_flag='N', payment_type='CRD', rate_code=1, pickup_longitude=-73.943106, pickup_latitude=40.706701, passenger_count=1, label=1.0, vector_numerico=DenseVector([-73.9431, 40.7067, 1.0]), vector_numerico_std=DenseVector([-8.6117, 8.5471, 0.719])),\n",
       " Row(vendor_id='CMT', store_and_fwd_flag='N', payment_type='CRD', rate_code=1, pickup_longitude=-74.006878, pickup_latitude=40.739494, passenger_count=1, label=2.4, vector_numerico=DenseVector([-74.0069, 40.7395, 1.0]), vector_numerico_std=DenseVector([-8.6192, 8.554, 0.719]))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_num.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos un pipeline para variables categoricas y otro para variables numericas creamos un pipeline que sea la union del output de ambas, dicho output lo llamamos por convencion `features`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensamblador_procesado = VectorAssembler(inputCols=[\"vector_numerico\", \"vector_categorico\"], \n",
    "                                         outputCol=\"features\")  \n",
    "\n",
    "pipeline_procesado = Pipeline(stages=[pipeline_numerico, pipeline_categorico, \n",
    "                                      ensamblador_procesado])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_procesado = pipeline_procesado.fit(taxi).transform(taxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(vendor_id='CMT', store_and_fwd_flag='N', payment_type='CRD', rate_code=1, pickup_longitude=-73.943106, pickup_latitude=40.706701, passenger_count=1, label=1.0, vector_numerico=DenseVector([-73.9431, 40.7067, 1.0]), vector_numerico_std=DenseVector([-8.6117, 8.5471, 0.719]), vendor_id_idx)=1.0, store_and_fwd_flag_idx)=0.0, payment_type_idx)=0.0, rate_code_idx)=0.0, vendor_id_enc=SparseVector(2, {1: 1.0}), store_and_fwd_flag_enc=SparseVector(2, {0: 1.0}), payment_type_enc=SparseVector(5, {0: 1.0}), rate_code_enc=SparseVector(12, {0: 1.0}), vector_categorico=SparseVector(21, {1: 1.0, 2: 1.0, 4: 1.0, 9: 1.0}), features=SparseVector(24, {0: -73.9431, 1: 40.7067, 2: 1.0, 4: 1.0, 5: 1.0, 7: 1.0, 12: 1.0})),\n",
       " Row(vendor_id='CMT', store_and_fwd_flag='N', payment_type='CRD', rate_code=1, pickup_longitude=-74.006878, pickup_latitude=40.739494, passenger_count=1, label=2.4, vector_numerico=DenseVector([-74.0069, 40.7395, 1.0]), vector_numerico_std=DenseVector([-8.6192, 8.554, 0.719]), vendor_id_idx)=1.0, store_and_fwd_flag_idx)=0.0, payment_type_idx)=0.0, rate_code_idx)=0.0, vendor_id_enc=SparseVector(2, {1: 1.0}), store_and_fwd_flag_enc=SparseVector(2, {0: 1.0}), payment_type_enc=SparseVector(5, {0: 1.0}), rate_code_enc=SparseVector(12, {0: 1.0}), vector_categorico=SparseVector(21, {1: 1.0, 2: 1.0, 4: 1.0, 9: 1.0}), features=SparseVector(24, {0: -74.0069, 1: 40.7395, 2: 1.0, 4: 1.0, 5: 1.0, 7: 1.0, 12: 1.0}))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_procesado.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora definimos el estimador, usaremos un modelo de regresion lineal por descenso de gradiente. \n",
    "\n",
    "Los estimadores de Spark, aparte de los hiperparámetros caracteristicos de cada algoritmo, tienen ciertos argumentos comunes:\n",
    "\n",
    "- `featuresCol`, el nombre de la columna de las variables independientes (por defecto, \"features\")\n",
    "- `labelCol`, el nombre de la columna de la variable independiente (por defecto, \"labels\")\n",
    "- `predictionCol`, el nombre para las predicciones (por defecto, \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(maxIter=100, regParam=0.3, elasticNetParam=0.8, \n",
    "                      featuresCol=\"features\", labelCol=\"label\",\n",
    "                      predictionCol=\"prediccion\")\n",
    "pipeline = Pipeline(stages=[pipeline_procesado, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hacemos `fit`, esto tarda unos 4 minutos en mi ordenador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 144 ms, sys: 29.3 ms, total: 173 ms\n",
      "Wall time: 2min 32s\n"
     ]
    }
   ],
   "source": [
    "%time modelo = pipeline.fit(taxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_d2e6153fce94"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora para obtener predicciones hacemos `transform`, no predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(vendor_id='CMT', store_and_fwd_flag='N', payment_type='CRD', rate_code=1, pickup_longitude=-73.943106, pickup_latitude=40.706701, passenger_count=1, label=1.0, vector_numerico=DenseVector([-73.9431, 40.7067, 1.0]), vector_numerico_std=DenseVector([-8.6117, 8.5471, 0.719]), vendor_id_idx)=1.0, store_and_fwd_flag_idx)=0.0, payment_type_idx)=0.0, rate_code_idx)=0.0, vendor_id_enc=SparseVector(2, {1: 1.0}), store_and_fwd_flag_enc=SparseVector(2, {0: 1.0}), payment_type_enc=SparseVector(5, {0: 1.0}), rate_code_enc=SparseVector(12, {0: 1.0}), vector_categorico=SparseVector(21, {1: 1.0, 2: 1.0, 4: 1.0, 9: 1.0}), features=SparseVector(24, {0: -73.9431, 1: 40.7067, 2: 1.0, 4: 1.0, 5: 1.0, 7: 1.0, 12: 1.0}), prediccion=2.1970520795492154)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.transform(taxi).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos guardar el output a otro archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.9 ms, sys: 15.1 ms, total: 49 ms\n",
      "Wall time: 42.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "(modelo\n",
    " .transform(taxi)\n",
    " .select(\n",
    "    variables_categoricas+variables_numericas+[\"prediccion\"]\n",
    " )\n",
    " .write\n",
    " .parquet(\"taxi_2014_prediccion\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_pred = spark.read.parquet(\"taxi_2014_prediccion/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(vendor_id='CMT', store_and_fwd_flag='N', payment_type='CRD', rate_code=1, pickup_longitude=-73.943106, pickup_latitude=40.706701, passenger_count=1, prediccion=2.1970520795492154)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(summary='count', prediccion='14999999'),\n",
       " Row(summary='mean', prediccion='1.4559072103899229'),\n",
       " Row(summary='stddev', prediccion='1.0965019704659398'),\n",
       " Row(summary='min', prediccion='0.2470778372101039'),\n",
       " Row(summary='max', prediccion='5.683756534354175')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_pred.select(\"prediccion\").describe().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validacion cruzada y Busquedas de hiperparámetros\n",
    "\n",
    "Spark es capaz de [validar y optimizar modelos](https://spark.apache.org/docs/latest/ml-tuning.html). Por desgracia no tiene la figura del RandomizedSearchCV, sin embargo si que tiene la busqueda en malla (`ParamGridBuilder`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [1e-3, 1.]) \\\n",
    "    .addGrid(lr.elasticNetParam, [1e-3, 1.]) \\\n",
    "    .build()\n",
    "type(paramGrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que hay 4 combinaciones distintas que va a buscar la malla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{Param(parent='LinearRegression_2aa128e77d15', name='regParam', doc='regularization parameter (>= 0).'): 0.001,\n",
       "  Param(parent='LinearRegression_2aa128e77d15', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.001},\n",
       " {Param(parent='LinearRegression_2aa128e77d15', name='regParam', doc='regularization parameter (>= 0).'): 0.001,\n",
       "  Param(parent='LinearRegression_2aa128e77d15', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 1.0},\n",
       " {Param(parent='LinearRegression_2aa128e77d15', name='regParam', doc='regularization parameter (>= 0).'): 1.0,\n",
       "  Param(parent='LinearRegression_2aa128e77d15', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.001},\n",
       " {Param(parent='LinearRegression_2aa128e77d15', name='regParam', doc='regularization parameter (>= 0).'): 1.0,\n",
       "  Param(parent='LinearRegression_2aa128e77d15', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 1.0}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paramGrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un evaluador de regression que sera el encargado de evaluar el funcionamiento de cada estimador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluador = RegressionEvaluator(metricName='rmse', predictionCol='prediccion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creamos un [`CrossValidator`](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator) que se encargue de la validacion cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluador,\n",
    "                          numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.tuning.CrossValidator"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(crossval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora entrenamos la busqueda, esto tarda unos 35 minutos en mi ordenador!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.1 s, sys: 1.13 s, total: 5.24 s\n",
      "Wall time: 27min 29s\n"
     ]
    }
   ],
   "source": [
    "%time cvModel = crossval.fit(taxi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez ha terminado podemos ver los resultados de cada iteracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.7172700256296567,\n",
       " 1.7172883634559573,\n",
       " 1.7372954404496626,\n",
       " 2.1302285248602355]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y podemos ver los resultados de cada combinacion de hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.7172700256296567,\n",
       "  {Param(parent='LinearRegression_2aa128e77d15', name='regParam', doc='regularization parameter (>= 0).'): 0.001,\n",
       "   Param(parent='LinearRegression_2aa128e77d15', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.001}),\n",
       " (1.7172883634559573,\n",
       "  {Param(parent='LinearRegression_2aa128e77d15', name='regParam', doc='regularization parameter (>= 0).'): 0.001,\n",
       "   Param(parent='LinearRegression_2aa128e77d15', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 1.0}),\n",
       " (1.7372954404496626,\n",
       "  {Param(parent='LinearRegression_2aa128e77d15', name='regParam', doc='regularization parameter (>= 0).'): 1.0,\n",
       "   Param(parent='LinearRegression_2aa128e77d15', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.001}),\n",
       " (2.1302285248602355,\n",
       "  {Param(parent='LinearRegression_2aa128e77d15', name='regParam', doc='regularization parameter (>= 0).'): 1.0,\n",
       "   Param(parent='LinearRegression_2aa128e77d15', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 1.0})]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(cvModel.avgMetrics, paramGrid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos  ver los pasos del pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PipelineModel_2e66bde88a1c, LinearRegression_2aa128e77d15]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.bestModel.stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y finalmente, podemos ver los hiperparametros del mejor estimador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LinearRegression_2aa128e77d15', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2)'): 2,\n",
       " Param(parent='LinearRegression_2aa128e77d15', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty'): 0.001,\n",
       " Param(parent='LinearRegression_2aa128e77d15', name='epsilon', doc='The shape parameter to control the amount of robustness. Must be > 1.0.'): 1.35,\n",
       " Param(parent='LinearRegression_2aa128e77d15', name='featuresCol', doc='features column name'): 'features',\n",
       " Param(parent='LinearRegression_2aa128e77d15', name='fitIntercept', doc='whether to fit an intercept term'): True,\n",
       " Param(parent='LinearRegression_2aa128e77d15', name='labelCol', doc='label column name'): 'label',\n",
       " Param(parent='LinearRegression_2aa128e77d15', name='loss', doc='The loss function to be optimized. Supported options: squaredError, huber. (Default squaredError)'): 'squaredError',\n",
       " Param(parent='LinearRegression_2aa128e77d15', name='maxIter', doc='maximum number of iterations (>= 0)'): 100,\n",
       " Param(parent='LinearRegression_2aa128e77d15', name='predictionCol', doc='prediction column name'): 'prediccion',\n",
       " Param(parent='LinearRegression_2aa128e77d15', name='regParam', doc='regularization parameter (>= 0)'): 0.001,\n",
       " Param(parent='LinearRegression_2aa128e77d15', name='solver', doc='The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (Default auto)'): 'auto',\n",
       " Param(parent='LinearRegression_2aa128e77d15', name='standardization', doc='whether to standardize the training features before fitting the model'): True,\n",
       " Param(parent='LinearRegression_2aa128e77d15', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0)'): 1e-06}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.bestModel.stages[1].extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
